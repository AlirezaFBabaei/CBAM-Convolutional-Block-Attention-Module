{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOQRFMIU7nB2"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "nA-uKLyTtuS_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXu_4kUtxjFh"
      },
      "source": [
        "# Channel Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JKRVXfPvuA8f"
      },
      "outputs": [],
      "source": [
        "class channel_attention_module(nn.Module):\n",
        "  def __init__(self, ch, ratio=8):\n",
        "    super().__init__()\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "    # Shared MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(ch, ch // ratio, bias = False),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(ch // ratio, ch, bias = False)\n",
        "    )\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.avg_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x1 = self.mlp(x1)\n",
        "\n",
        "    x2 = self.max_pool(x).squeeze(-1).squeeze(-1)\n",
        "    x2 = self.mlp(x2)\n",
        "\n",
        "    # Add Average and Max Pooling\n",
        "    concat = x1 + x2\n",
        "    concat = self.sigmoid(concat).unsqueeze(-1).unsqueeze(-1)\n",
        "    refined_feats = x * concat\n",
        "\n",
        "    return refined_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU44whtJxmf0"
      },
      "source": [
        "# Spatial Attention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lL52V7Ga2ZJx"
      },
      "outputs": [],
      "source": [
        "class spatial_attention_module(nn.Module):\n",
        "  def __init__(self, kernel_size = 7):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(2, 1, kernel_size, padding = 3, bias = False)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Average Pooling\n",
        "    x1 = torch.mean(x, dim = 1, keepdim = True)\n",
        "    # Max Pooling\n",
        "    x2, _ = torch.max(x, dim = 1, keepdim = True)\n",
        "\n",
        "    # Concatenate Average and Max Pooling\n",
        "    concat = torch.cat([x1, x2], dim = 1)\n",
        "\n",
        "    concat= self.conv(concat)\n",
        "    concat = self.sigmoid(concat)\n",
        "\n",
        "    refined_feats = x * concat\n",
        "\n",
        "    return refined_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_ZW5Aj6KKE"
      },
      "source": [
        "# CBAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gNJ6Cdyv6X8I"
      },
      "outputs": [],
      "source": [
        "class CBAM(nn.Module):\n",
        "  def __init__(self, channel):\n",
        "    super().__init__()\n",
        "\n",
        "    self.channel_attention = channel_attention_module(channel)\n",
        "    self.spatial_attention = spatial_attention_module()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.channel_attention(x)\n",
        "    x = self.spatial_attention(x)\n",
        "\n",
        "    return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
